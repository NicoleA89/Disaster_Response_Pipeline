{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ML Pipeline Preparation\n## 1. Import libraries and load data from database."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import python libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom pprint import pprint\n\nimport re\nimport sys\n\n\n#Import sklearn\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sqlalchemy import create_engine\n\n# Output of the kaggle data sources\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data from database\nengine = create_engine('sqlite:////kaggle/input/disasterresponse/DisasterResponse.db')\ndf = pd.read_sql_table('DisasterResponse', con=engine)\n\ncategories = df.columns[4:]\n\nX = df[['message']].values[:, 0]\ny = df[categories].values\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the first line of 'X'\nX[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the first line of 'y'\ny[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Normalize, lemmatize and tokenize text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get an overview over the English stopwords\nprint(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define tokenize function to reduce message complexity\n\nurl_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n\ndef tokenize(text):\n    \"\"\"\n    Tokenizes text data\n    \n    Arguments:\n    text str: Messages as text data\n    \n    Returns:\n    clean_tokens: Processed text after normalizing, tokenizing and lemmatizing\n    \"\"\"\n        \n    # Detect URLs\n    detected_urls = re.findall(url_regex, text)\n    for url in detected_urls:\n        text = text.replace(url, 'urlplaceholder')\n        \n    # Normalize and tokenize\n    tokens = nltk.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()))\n    \n    # Remove stopwords\n    tokens = [t for t in tokens if t not in stopwords.words('english')]\n\n    lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize\n    clean_tokens = []\n    for tok in tokens:\n        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n        clean_tokens.append(clean_tok)\n    \n    return clean_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the results after tokenization\nfor message in X[:6]:\n    tokens = tokenize(message)\n    print(message)\n    print(tokens, '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Build a machine learning pipeline"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Setup a machine learning pipeline\npipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultiOutputClassifier(RandomForestClassifier(class_weight='balanced')))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Train pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset into test and training parts\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n# Train classifier\npipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Test model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report accurancy, precision, recall and f1 score for each output category of the dataset.\n\ndef classification_report_output(y_true, y_pred):\n    \"\"\"\n    Outputs accuracy, precision, recall and f1 score for each output category of the dataset.\n    \n    Arguments:\n    y_true\n    y_pred\n    \"\"\"\n        \n    for i in range(0, len(categories)):\n        print(categories[i])\n        print(\"\\tAccuracy: {:.4f}\\t\\t% Precision: {:.4f}\\t\\t% Recall: {:.4f}\\t\\t% F1_score: {:.4f}\".format(\n            accuracy_score(y_true[:, i], y_pred[:, i]),\n            precision_score(y_true[:, i], y_pred[:, i], average='weighted'),\n            recall_score(y_true[:, i], y_pred[:, i], average='weighted'),\n            f1_score(y_true[:, i], y_pred[:, i], average='weighted')\n        ))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report metrics for training model\ny_pred = pipeline.predict(X_train)\nclassification_report_output(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report metrics for test model\ny_pred = pipeline.predict(X_test)\nclassification_report_output(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Improve model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use grid search to find best parameters\nparameters = {'vect__ngram_range': ((1, 1), (1, 2)),\n              'vect__max_df': (0.75, 1.0)\n              }\n\ncv = GridSearchCV(estimator=pipeline, param_grid=parameters)\ncv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report metrics for training model\ny_pred = pipeline.predict(X_train)\nclassification_report_output(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report metrics for test model\ny_pred = pipeline.predict(X_test)\nclassification_report_output(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Export the model as a pickle file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Export model\nwith open('adaboost_cv.pkl', 'wb') as file:\n    pickle.dump(cv, file)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}