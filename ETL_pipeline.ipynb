{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ETL Pipeline Preparation\n## 1. Import libraries and load datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n# Output of the kaggle data sources\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load messages dataset\nmessages = pd.read_csv(\"/kaggle/input/disasterresponse/disaster_messages.csv\")\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load categories dataset\ncategories = pd.read_csv(\"/kaggle/input/disasterresponse/disaster_categories.csv\")\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at the shape of each dataset\nprint(messages.shape)\nprint(categories.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Combine Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Merge the datasets together based on their common column 'id'\ndf = messages.merge(categories, on='id')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Split categories into separate category columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new dataset of the 36 individual category columns and separate the solumns based on the charakter ;\ncategories = df['categories'].str.split(\";\",expand = True)\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the first row of the categories dataframe\nrow = categories.iloc[0,:].values\n\n# Use this row to extract a list of new column names for categories\ncategory_colnames = [r[:-2] for r in row]\nprint(category_colnames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the columns of `categories`\ncategories.columns = category_colnames\ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Convert category values to numbers 0 or 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert category values to just numbers 0 or 1\nfor column in categories:\n \n    # Set each value to be only the last character of the string\n    categories[column] = categories[column].str[-1]\n    \n    # Convert column from string to numeric\n    categories[column] = pd.to_numeric(categories[column])\n    \ncategories.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Replace categories column in the original dataset with new category columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the original categories column from `df`\ndf.drop('categories', axis = 1, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the new `categories` columns\ndf = pd.concat([df, categories], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Remove duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the number of duplicates\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the duplicates\ndf.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the number of duplicates agein\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Save the clean dataset into an sqlite database"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using pandas to_sql method to create an sqlite database\nengine = create_engine('sqlite:///DisasterResponse.db')\ndf.to_sql('DisasterResponse', engine, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. With this notebook complete process_data.py\nFor the result, see workspace/data/process_data.py"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}